{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import models\n",
    "import torch.distributed as dist\n",
    "from data import DataRegime\n",
    "from utils.log import setup_logging, ResultsLog, save_checkpoint\n",
    "from utils.optim import OptimRegime\n",
    "from utils.cross_entropy import CrossEntropyLoss\n",
    "from utils.misc import torch_dtypes\n",
    "from utils.param_filter import FilterModules, is_bn\n",
    "from utils.convert_pytcv_model import convert_pytcv_model\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from trainer import Trainer\n",
    "from utils.adaquant import *\n",
    "import torchvision\n",
    "import scipy.optimize as opt\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import numpy as np\n",
    "from models.modules.quantize import methods\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import shutil\n",
    "from models.modules.quantize import QParams\n",
    "import ast\n",
    "import ntpath\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global best_prec1, dtype\n",
    "    acc = -1\n",
    "    loss = -1\n",
    "    best_prec1 = 0\n",
    "    dtype = torch_dtypes.get(args.dtype)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    if args.evaluate:\n",
    "        args.results_dir = '/tmp'\n",
    "    if args.save is '':\n",
    "        args.save = time_stamp\n",
    "    save_path = os.path.join(args.results_dir, args.save)\n",
    "\n",
    "    args.distributed = args.local_rank >= 0 or args.world_size > 1\n",
    "\n",
    "    if args.distributed:\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_init,\n",
    "                                world_size=args.world_size, rank=args.local_rank)\n",
    "        args.local_rank = dist.get_rank()\n",
    "        args.world_size = dist.get_world_size()\n",
    "        if args.dist_backend == 'mpi':\n",
    "            # If using MPI, select all visible devices\n",
    "            args.device_ids = list(range(torch.cuda.device_count()))\n",
    "        else:\n",
    "            args.device_ids = [args.local_rank]\n",
    "\n",
    "    if not os.path.exists(save_path) and not (args.distributed and args.local_rank > 0):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    setup_logging(os.path.join(save_path, 'log.txt'),\n",
    "                  resume=args.resume is not '',\n",
    "                  dummy=args.distributed and args.local_rank > 0)\n",
    "\n",
    "    results_path = os.path.join(save_path, 'results')\n",
    "    results = ResultsLog(\n",
    "        results_path, title='Training Results - %s' % args.save)\n",
    "\n",
    "    logging.info(\"saving to %s\", save_path)\n",
    "    logging.debug(\"run arguments: %s\", args)\n",
    "    logging.info(\"creating model %s\", args.model)\n",
    "\n",
    "    if 'cuda' in args.device and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        torch.cuda.set_device(args.device_ids[0])\n",
    "        cudnn.benchmark = True\n",
    "    else:\n",
    "        args.device_ids = None\n",
    "\n",
    "    # create model\n",
    "    model = models.__dict__[args.model]\n",
    "    dataset_type = 'imagenet' if args.dataset =='imagenet_calib' else args.dataset\n",
    "    model_config = {'dataset': dataset_type}\n",
    "\n",
    "    if args.model_config is not '':\n",
    "        if isinstance(args.model_config, dict):\n",
    "            for k, v in args.model_config.items():\n",
    "                if k not in model_config.keys():\n",
    "                    model_config[k] = v\n",
    "        else:\n",
    "            args_dict = literal_eval(args.model_config)\n",
    "            for k, v in args_dict.items():\n",
    "                model_config[k] = v\n",
    "    if (args.absorb_bn or args.load_from_vision or args.pretrained) and not args.batch_norn_tuning:\n",
    "        if args.load_from_vision:\n",
    "            import torchvision\n",
    "            exec_lfv_str = 'torchvision.models.' + args.load_from_vision + '(pretrained=True)'\n",
    "            model = eval(exec_lfv_str)\n",
    "            if 'pytcv' in args.model:\n",
    "                from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "                exec_lfv_str ='ptcv_get_model(\"'+ args.load_from_vision +'\", pretrained=True)'\n",
    "                model_pytcv = eval(exec_lfv_str)\n",
    "                model = convert_pytcv_model(model,model_pytcv)\n",
    "        else:\n",
    "            if not os.path.isfile(args.absorb_bn):\n",
    "                parser.error('invalid checkpoint: {}'.format(args.evaluate))\n",
    "            model = model(**model_config)\n",
    "            checkpoint = torch.load(args.absorb_bn,map_location=lambda storage, loc: storage)\n",
    "            checkpoint = checkpoint['state_dict'] if 'state_dict' in checkpoint.keys() else checkpoint\n",
    "            model.load_state_dict(checkpoint,strict=False)\n",
    "        if 'batch_norm' in model_config and not model_config['batch_norm']:\n",
    "            logging.info('Creating absorb_bn state dict')\n",
    "            search_absorbe_bn(model)\n",
    "            filename_ab = args.absorb_bn+'.absorb_bn' if args.absorb_bn else save_path+'/'+args.model+'.absorb_bn'\n",
    "            torch.save(model.state_dict(),filename_ab)\n",
    "        else:    \n",
    "            filename_bn = save_path+'/'+args.model+'.with_bn'\n",
    "            torch.save(model.state_dict(),filename_bn)\n",
    "        if (args.load_from_vision or args.absorb_bn) and not args.evaluate_init_configuration: return\n",
    "\n",
    "    if 'inception' in args.model:\n",
    "        model = model(init_weights=False, **model_config)\n",
    "    else:\n",
    "        model = model(**model_config)\n",
    "    logging.info(\"created model with configuration: %s\", model_config)\n",
    "    \n",
    "    num_parameters = sum([l.nelement() for l in model.parameters()])\n",
    "    logging.info(\"number of parameters: %d\", num_parameters)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.evaluate:\n",
    "        if not os.path.isfile(args.evaluate):\n",
    "            parser.error('invalid checkpoint: {}'.format(args.evaluate))\n",
    "        checkpoint = torch.load(args.evaluate, map_location=\"cpu\")\n",
    "        # Overrride configuration with checkpoint info\n",
    "        args.model = checkpoint.get('model', args.model)\n",
    "        args.model_config = checkpoint.get('config', args.model_config)\n",
    "        if not model_config['batch_norm']:\n",
    "            search_absorbe_fake_bn(model)\n",
    "        # load checkpoint\n",
    "        if 'state_dict' in checkpoint.keys():\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            logging.info(\"loaded checkpoint '%s'\", args.evaluate)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint,strict=False)\n",
    "            logging.info(\"loaded checkpoint '%s'\",args.evaluate)\n",
    "          \n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint_file = args.resume\n",
    "        if os.path.isdir(checkpoint_file):\n",
    "            results.load(os.path.join(checkpoint_file, 'results.csv'))\n",
    "            checkpoint_file = os.path.join(\n",
    "                checkpoint_file, 'model_best.pth.tar')\n",
    "        if os.path.isfile(checkpoint_file):\n",
    "            logging.info(\"loading checkpoint '%s'\", args.resume)\n",
    "            checkpoint = torch.load(checkpoint_file)\n",
    "            if args.start_epoch < 0:  # not explicitly set\n",
    "                args.start_epoch = checkpoint['epoch'] - 1 if 'epoch' in checkpoint.keys() else 0    \n",
    "            best_prec1 = checkpoint['best_prec1'] if 'best_prec1' in checkpoint.keys() else -1\n",
    "            sd = checkpoint['state_dict'] if 'state_dict' in checkpoint.keys() else checkpoint\n",
    "            model.load_state_dict(sd,strict=False)\n",
    "            logging.info(\"loaded checkpoint '%s' (epoch %s)\",\n",
    "                         checkpoint_file, args.start_epoch)\n",
    "        else:\n",
    "            logging.error(\"no checkpoint found at '%s'\", args.resume)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    loss_params = {}\n",
    "    if args.label_smoothing > 0:\n",
    "        loss_params['smooth_eps'] = args.label_smoothing\n",
    "    criterion = getattr(model, 'criterion', CrossEntropyLoss)(**loss_params)\n",
    "    if args.kld_loss:\n",
    "       criterion = nn.KLDivLoss(reduction='mean') \n",
    "    criterion.to(args.device, dtype)\n",
    "    model.to(args.device, dtype)\n",
    "\n",
    "    # Batch-norm should always be done in float\n",
    "    if 'half' in args.dtype:\n",
    "        FilterModules(model, module=is_bn).to(dtype=torch.float)\n",
    "\n",
    "    # optimizer configuration\n",
    "    optim_regime = getattr(model, 'regime', [{'epoch': 0,\n",
    "                                              'optimizer': args.optimizer,\n",
    "                                              'lr': args.lr,\n",
    "                                              'momentum': args.momentum,\n",
    "                                              'weight_decay': args.weight_decay}])\n",
    "    if args.fine_tune or args.prune: \n",
    "        if not args.resume: args.start_epoch=0  \n",
    "        if args.update_only_th:\n",
    "            #optim_regime = [\n",
    "            #    {'epoch': 0, 'optimizer': 'Adam', 'lr': 1e-4}] \n",
    "            optim_regime = [\n",
    "                {'epoch': 0, 'optimizer': 'SGD', 'lr': 1e-1},\n",
    "                {'epoch': 10, 'lr': 1e-2},\n",
    "                {'epoch': 15, 'lr': 1e-3}]\n",
    "        else:              \n",
    "            optim_regime = [\n",
    "                {'epoch': 0, 'optimizer': 'SGD', 'lr': 1e-4, 'momentum': 0.9},\n",
    "                {'epoch': 2, 'lr': 1e-5, 'momentum': 0.9},\n",
    "                {'epoch': 10, 'lr': 1e-6, 'momentum': 0.9}]\n",
    "    optimizer = optim_regime if isinstance(optim_regime, OptimRegime) \\\n",
    "        else OptimRegime(model, optim_regime, use_float_copy='half' in args.dtype)\n",
    "\n",
    "    # Training Data loading code\n",
    "    \n",
    "    train_data = DataRegime(getattr(model, 'data_regime', None),\n",
    "                            defaults={'datasets_path': args.datasets_dir, 'name': args.dataset, 'split': 'train', 'augment': False,\n",
    "                                      'input_size': args.input_size,  'batch_size': args.batch_size, 'shuffle': not args.seq_adaquant,\n",
    "                                      'num_workers': args.workers, 'pin_memory': True, 'drop_last': True,\n",
    "                                      'distributed': args.distributed, 'duplicates': args.duplicates, 'autoaugment': args.autoaugment,\n",
    "                                      'cutout': {'holes': 1, 'length': 16} if args.cutout else None})\n",
    "    if args.names_sp_layers is None and args.layers_precision_dict is None:\n",
    "        args.names_sp_layers =  [key[:-7] for key in model.state_dict().keys() if 'weight' in key and 'running' not in key and ('conv' in key or 'downsample.0' in key or 'fc' in key)]\n",
    "        if args.keep_first_last: args.names_sp_layers=[name for name in args.names_sp_layers if name!='conv1' and name!='fc' and name != 'Conv2d_1a_3x3.conv']\n",
    "        args.names_sp_layers = [k for k in args.names_sp_layers if 'downsample' not in k] if args.ignore_downsample else args.names_sp_layers\n",
    "        if args.num_sp_layers == 0 and not args.keep_first_last:\n",
    "            args.names_sp_layers = []\n",
    "\n",
    "    prunner = None \n",
    "    trainer = Trainer(model,prunner, criterion, optimizer,\n",
    "                      device_ids=args.device_ids, device=args.device, dtype=dtype,\n",
    "                      distributed=args.distributed, local_rank=args.local_rank, mixup=args.mixup, loss_scale=args.loss_scale,\n",
    "                      grad_clip=args.grad_clip, print_freq=args.print_freq, adapt_grad_norm=args.adapt_grad_norm,epoch=args.start_epoch,update_only_th=args.update_only_th,optimize_rounding=args.optimize_rounding)\n",
    "\n",
    "    \n",
    "    # Evaluation Data loading code\n",
    "    args.eval_batch_size = args.eval_batch_size if args.eval_batch_size > 0 else args.batch_size     \n",
    "    dataset_type = 'imagenet' if args.dataset =='imagenet_calib' else args.dataset\n",
    "    val_data = DataRegime(getattr(model, 'data_eval_regime', None),\n",
    "                          defaults={'datasets_path': args.datasets_dir, 'name': dataset_type, 'split': 'val', 'augment': False,\n",
    "                                    'input_size': args.input_size, 'batch_size': args.eval_batch_size, 'shuffle': True,\n",
    "                                    'num_workers': args.workers, 'pin_memory': True, 'drop_last': False})\n",
    "\n",
    "    if args.evaluate or args.resume:\n",
    "        from utils.layer_sensativity import search_replace_layer , extract_save_quant_state_dict, search_replace_layer_from_dict\n",
    "        if args.layers_precision_dict is not None:\n",
    "            model = search_replace_layer_from_dict(model, ast.literal_eval(args.layers_precision_dict))\n",
    "        else:\n",
    "            model = search_replace_layer(model, args.names_sp_layers, num_bits_activation=args.nbits_act,\n",
    "                                         num_bits_weight=args.nbits_weight)\n",
    "\n",
    "    cached_input_output = {}\n",
    "    quant_keys = ['.weight', '.bias', '.equ_scale', '.quantize_input.running_zero_point', '.quantize_input.running_range',\n",
    "         '.quantize_weight.running_zero_point', '.quantize_weight.running_range','.quantize_input1.running_zero_point', '.quantize_input1.running_range'\n",
    "         '.quantize_input2.running_zero_point', '.quantize_input2.running_range']        \n",
    "    if args.adaquant:\n",
    "        def Qhook(name,module, input, output):\n",
    "            if module not in cached_qinput:\n",
    "                cached_qinput[module] = []\n",
    "            # Meanwhile store data in the RAM.\n",
    "            cached_qinput[module].append(input[0].detach().cpu())\n",
    "            # print(name)\n",
    "\n",
    "        def hook(name,module, input, output):\n",
    "            if module not in cached_input_output:\n",
    "                cached_input_output[module] = []\n",
    "            # Meanwhile store data in the RAM.\n",
    "            cached_input_output[module].append((input[0].detach().cpu(), output.detach().cpu()))\n",
    "            # print(name)\n",
    "\n",
    "        from models.modules.quantize import QConv2d, QLinear\n",
    "        handlers = []\n",
    "        count = 0\n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
    "            #if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
    "            # if isinstance(m, QConv2d):\n",
    "                m.quantize = False\n",
    "                if count < 1000:\n",
    "                # if (isinstance(m, QConv2d) and m.groups == 1) or isinstance(m, QLinear):\n",
    "                    handlers.append(m.register_forward_hook(partial(hook,name)))\n",
    "                    count += 1\n",
    "\n",
    "        # Store input/output for all quantizable layers\n",
    "        trainer.validate(train_data.get_loader())\n",
    "        print(\"Input/outputs cached\")\n",
    "\n",
    "        for handler in handlers:\n",
    "            handler.remove()\n",
    "\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
    "                m.quantize = True\n",
    "\n",
    "        mse_df = pd.DataFrame(index=np.arange(len(cached_input_output)), columns=['name', 'bit', 'shape', 'mse_before', 'mse_after'])\n",
    "        print_freq = 100\n",
    "        for i, layer in enumerate(cached_input_output):\n",
    "            if i>0 and args.seq_adaquant:\n",
    "                count = 0\n",
    "                cached_qinput = {}\n",
    "                for name, m in model.named_modules():\n",
    "                    if layer.name==name:\n",
    "                        if count < 1000:\n",
    "                            handler= m.register_forward_hook(partial(Qhook,name))\n",
    "                            count += 1\n",
    "                # Store input/output for all quantizable layers\n",
    "                trainer.validate(train_data.get_loader())\n",
    "                print(\"cashed quant Input%s\"%layer.name)\n",
    "                cached_input_output[layer][0] = (cached_qinput[layer][0],cached_input_output[layer][0][1])\n",
    "                handler.remove()            \n",
    "            print(\"\\nOptimize {}:{} for {} bit of shape {}\".format(i, layer.name, layer.num_bits, layer.weight.shape))\n",
    "            mse_before, mse_after, snr_before, snr_after, kurt_in, kurt_w = \\\n",
    "                optimize_layer(layer, cached_input_output[layer], args.optimize_weights, batch_size=args.batch_size, model_name=args.model)\n",
    "            print(\"\\nMSE before optimization: {}\".format(mse_before))\n",
    "            print(\"MSE after optimization:  {}\".format(mse_after))\n",
    "            mse_df.loc[i, 'name'] = layer.name\n",
    "            mse_df.loc[i, 'bit'] = layer.num_bits\n",
    "            mse_df.loc[i, 'shape'] = str(layer.weight.shape)\n",
    "            mse_df.loc[i, 'mse_before'] = mse_before\n",
    "            mse_df.loc[i, 'mse_after'] = mse_after\n",
    "            mse_df.loc[i, 'snr_before'] = snr_before\n",
    "            mse_df.loc[i, 'snr_after'] = snr_after\n",
    "            mse_df.loc[i, 'kurt_in'] = kurt_in\n",
    "            mse_df.loc[i, 'kurt_w'] = kurt_w\n",
    "\n",
    "        mse_csv = args.evaluate + '.mse.csv'\n",
    "        mse_df.to_csv(mse_csv)\n",
    "\n",
    "        filename = args.evaluate + '.adaquant'\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "        train_data = None\n",
    "        cached_input_output = None\n",
    "        val_results = trainer.validate(val_data.get_loader())\n",
    "        logging.info(val_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
