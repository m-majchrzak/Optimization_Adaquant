{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-majchrzak/Optimization_Adaquant/blob/main/main_mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cyu_LNHkLcuQ"
      },
      "source": [
        "## Libraries & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gsj6-ZzeN5_",
        "outputId": "1c92350a-64f8-4dab-8750-420d2777da39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "rm: cannot remove './Optimization_Adaquant/': No such file or directory\n",
            "Cloning into 'Optimization_Adaquant'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 128 (delta 62), reused 70 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 6.31 MiB | 20.72 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "/content/Optimization_Adaquant\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyunpack\n",
            "  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n",
            "Collecting easyprocess (from pyunpack)\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting entrypoint2 (from pyunpack)\n",
            "  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: entrypoint2, easyprocess, pyunpack\n",
            "Successfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\n"
          ]
        }
      ],
      "source": [
        "# %cd /content/\n",
        "# !rm -r ./Optimization_Adaquant/\n",
        "# !git clone https://github.com/m-majchrzak/Optimization_Adaquant.git\n",
        "# %cd Optimization_Adaquant/\n",
        "# !pip install pyunpack\n",
        "# from pyunpack import Archive\n",
        "# Archive('calibration_datasets.zip').extractall(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LWJOoH8xLcuU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.parallel\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "from utils.adaquant import optimize_layer_adaquant\n",
        "from utils.load_dataset import load_dataset\n",
        "from utils.misc import set_global_seeds\n",
        "from utils.mobilenet_v2 import mobilenet_v2, MobileNetV2\n",
        "from utils.quantize import QConv2d, QLinear\n",
        "from utils.resnet import ResNet_imagenet\n",
        "from utils.trainer import Trainer\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from utils.kaggle_cifar_10_dataset import KaggleCIFAR10Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nsPl6gZlLcuW"
      },
      "outputs": [],
      "source": [
        "acc = -1\n",
        "loss = -1\n",
        "best_prec1 = 0\n",
        "dtype = torch.float32\n",
        "\n",
        "### SET SEED\n",
        "seed = 123\n",
        "set_global_seeds(seed)\n",
        "\n",
        "device_ids = list(range(torch.cuda.device_count()))\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(device_ids[0])\n",
        "    cudnn.benchmark = True\n",
        "else:\n",
        "    device_ids = None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sjIYB-L8LcuX"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZtCWrcg2LcuY"
      },
      "outputs": [],
      "source": [
        "# # Calib data loading code\n",
        "# calib_directory = \"./data/calibration\"\n",
        "# calib_batch_size = 100\n",
        "# calib_data = load_dataset(calib_directory, calib_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Train data loading code\n",
        "# train_directory = \"./data/train\"\n",
        "# train_batch_size = 100\n",
        "# train_data = load_dataset(train_directory, train_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def predict_from_dataloader(dataloader, model):\n",
        "#     # model.eval()\n",
        "#     # model.to(device)\n",
        "#     with torch.no_grad():\n",
        "#         for i, (inputs, labels) in enumerate(dataloader):\n",
        "#             # inputs, labels = data\n",
        "#             # inputs = inputs.to(device)\n",
        "#             # labels = labels.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             if i == 0:\n",
        "#                 all_predicted = predicted\n",
        "#                 all_labels = labels\n",
        "#             else:\n",
        "#                 all_predicted = torch.cat((all_predicted, predicted), 0)\n",
        "#                 all_labels = torch.cat((all_labels, labels), 0)\n",
        "#     return all_predicted, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_loop(dataloader, model, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    n_batches = len(dataloader)\n",
        "    val_loss, score = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_imgs, batch_labels in dataloader:\n",
        "            batch_imgs, batch_labels = batch_imgs.to(device), batch_labels.to(device)\n",
        "            logits = model(batch_imgs)\n",
        "            # val_loss += loss_fn(logits, batch_labels).item()\n",
        "            score += (logits.argmax(1) == batch_labels).type(torch.float).sum().item()\n",
        "\n",
        "    # val_loss /= n_batches\n",
        "    score /= size\n",
        "    accuracy = 100 * score\n",
        "    # print(f'Validation error: \\n Accuracy: {(accuracy):>0.1f}, Avg loss: {val_loss:>8f}\\n')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seeds(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = KaggleCIFAR10Dataset(\n",
        "        'data/calibration_Wladek', \n",
        "        'data/calibration_labels.csv', \n",
        "         transforms.Compose([ # basic augmentation\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ColorJitter(),\n",
        "                transforms.RandomRotation(10)\n",
        "            ]))\n",
        "\n",
        "cal_dataloader, _ = dataset.get_train_val_dataloaders(\n",
        "    0.999, \n",
        "    {\n",
        "        'batch_size': 128,\n",
        "        'shuffle': False,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = KaggleCIFAR10Dataset(\n",
        "        'cifar-10/train', \n",
        "        'cifar-10/trainLabels.csv', \n",
        "         transforms.Compose([ # basic augmentation\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ColorJitter(),\n",
        "                transforms.RandomRotation(10)\n",
        "            ]))\n",
        "\n",
        "train_dataloader, val_dataloader = dataset.get_train_val_dataloaders(\n",
        "    0.9, \n",
        "    {\n",
        "        'batch_size': 128,\n",
        "        'shuffle': True,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = MobileNetV2(num_bits=8, num_bits_weight=8)\n",
        "model.load_state_dict(torch.load('./mobilenet1.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.707707707707708"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_loop(cal_dataloader, model, 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loop(val_dataloader, model, 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4)\n",
            "tensor(9)\n",
            "tensor(8)\n",
            "tensor(3)\n",
            "tensor(5)\n",
            "tensor(3)\n",
            "tensor(3)\n",
            "tensor(6)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for img, labels in cal_dataloader:\n",
        "    plt.imshow(img[0].permute(1,2,0))\n",
        "    print(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'frog': 0,\n",
              " 'truck': 1,\n",
              " 'deer': 2,\n",
              " 'automobile': 3,\n",
              " 'bird': 4,\n",
              " 'horse': 5,\n",
              " 'ship': 6,\n",
              " 'cat': 7,\n",
              " 'dog': 8,\n",
              " 'airplane': 9}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.get_labels_mapping()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45.00900180036007"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_loop(val_dataloader, model, 'cpu')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WyZ0-rUnLcua"
      },
      "source": [
        "## Model, Optimizer, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_VyDDpZPLcua"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\wlady/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
          ]
        }
      ],
      "source": [
        "### CREATE MODEL\n",
        "# model = ResNet_imagenet()\n",
        "# model = mobilenet_v2()  # num_bits=32, num_bits_weight=32)\n",
        "\n",
        "# # define loss function (criterion)\n",
        "# criterion = CrossEntropyLoss()\n",
        "\n",
        "# # optimizer configuration\n",
        "# optimizer = SGD(model.parameters(), lr=1e-2, momentum=0.5, weight_decay=0)\n",
        "\n",
        "# TRAINER\n",
        "# trainer = Trainer(model, criterion, optimizer, device=torch.device('cuda'))\n",
        "# trainer = Trainer(model, criterion, optimizer, device=torch.device(\"cpu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "### LOAD MODEL\n",
        "# model = MobileNetV2(num_bits=32, num_bits_weight=32)\n",
        "# model.load_state_dict(torch.load('./mobilenet1.pt'))\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# optimizer configuration\n",
        "optimizer = SGD(model.parameters(), lr=1e-2, momentum=0.5, weight_decay=0)\n",
        "trainer = Trainer(model, criterion, optimizer, device=torch.device(\"cpu\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y1QO1EtnLcuc"
      },
      "source": [
        "## Cache, Hook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "llDYo3MNLcuc"
      },
      "outputs": [],
      "source": [
        "cached_qinput = {}\n",
        "\n",
        "\n",
        "def Qhook(name, module, input, output):\n",
        "    if module not in cached_qinput:\n",
        "        cached_qinput[module] = []\n",
        "        # Meanwhile store data in the RAM.\n",
        "        cached_qinput[module].append(input[0].detach().cpu())\n",
        "        # print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_y_PDu3wLcud"
      },
      "outputs": [],
      "source": [
        "cached_input_output = {}\n",
        "\n",
        "\n",
        "def hook(name, module, input, output):\n",
        "    if module not in cached_input_output:\n",
        "        cached_input_output[module] = []\n",
        "    # Meanwhile store data in the RAM.\n",
        "    cached_input_output[module].append((input[0].detach().cpu(), output.detach().cpu()))\n",
        "    # print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl8zpIswLcud",
        "outputId": "c7025283-f196-4233-ac6a-f7c338988bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features.0.0\n",
            "features.1.conv.0.0\n",
            "features.1.conv.1\n",
            "features.2.conv.0.0\n",
            "features.2.conv.1.0\n",
            "features.2.conv.2\n",
            "features.3.conv.0.0\n",
            "features.3.conv.1.0\n",
            "features.3.conv.2\n",
            "features.4.conv.0.0\n",
            "features.4.conv.1.0\n",
            "features.4.conv.2\n",
            "features.5.conv.0.0\n",
            "features.5.conv.1.0\n",
            "features.5.conv.2\n",
            "features.6.conv.0.0\n",
            "features.6.conv.1.0\n",
            "features.6.conv.2\n",
            "features.7.conv.0.0\n",
            "features.7.conv.1.0\n",
            "features.7.conv.2\n",
            "features.8.conv.0.0\n",
            "features.8.conv.1.0\n",
            "features.8.conv.2\n",
            "features.9.conv.0.0\n",
            "features.9.conv.1.0\n",
            "features.9.conv.2\n",
            "features.10.conv.0.0\n",
            "features.10.conv.1.0\n",
            "features.10.conv.2\n",
            "features.11.conv.0.0\n",
            "features.11.conv.1.0\n",
            "features.11.conv.2\n",
            "features.12.conv.0.0\n",
            "features.12.conv.1.0\n",
            "features.12.conv.2\n",
            "features.13.conv.0.0\n",
            "features.13.conv.1.0\n",
            "features.13.conv.2\n",
            "features.14.conv.0.0\n",
            "features.14.conv.1.0\n",
            "features.14.conv.2\n",
            "features.15.conv.0.0\n",
            "features.15.conv.1.0\n",
            "features.15.conv.2\n",
            "features.16.conv.0.0\n",
            "features.16.conv.1.0\n",
            "features.16.conv.2\n",
            "features.17.conv.0.0\n",
            "features.17.conv.1.0\n",
            "features.17.conv.2\n",
            "features.18.0\n",
            "classifier.1\n"
          ]
        }
      ],
      "source": [
        "for name, m in model.named_modules():\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        print(name)\n",
        "        m.name = name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jQIYG_5dLcue"
      },
      "outputs": [],
      "source": [
        "handlers = []\n",
        "count = 0\n",
        "for name, m in model.named_modules():\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        # if isinstance(m, Conv2d) or isinstance(m, Linear):\n",
        "        # if isinstance(m, QConv2d):\n",
        "        m.quantize = False\n",
        "        # if count < 10:\n",
        "        # if (isinstance(m, QConv2d) and m.groups == 1) or isinstance(m, QLinear):\n",
        "        handlers.append(m.register_forward_hook(partial(hook, name)))\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GrF88RKLcuf",
        "outputId": "9c35c806-25f9-44c2-c0a0-8ea580087aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input/outputs cached\n"
          ]
        }
      ],
      "source": [
        "# Store input/output for all quantizable layers\n",
        "trainer.validate(val_dataloader)\n",
        "print(\"Input/outputs cached\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UUS3WM1TLcuf"
      },
      "outputs": [],
      "source": [
        "for handler in handlers:\n",
        "    handler.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7FUABnD0Lcug"
      },
      "outputs": [],
      "source": [
        "for m in model.modules():\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        m.quantize = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbfo-MoXLcug",
        "outputId": "540dcbf2-fdd8-4236-d8b7-92f2d2a837cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys([QConv2d(\n",
            "  3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QLinear(\n",
            "  in_features=1280, out_features=10, bias=True\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")])\n"
          ]
        }
      ],
      "source": [
        "print(cached_input_output.keys())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3FiBDSMdLcuh"
      },
      "source": [
        "## Loop Through Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "AjR0_09hLcuh"
      },
      "outputs": [],
      "source": [
        "mse_df = pd.DataFrame(\n",
        "    index=np.arange(len(cached_input_output)),\n",
        "    columns=[\"name\", \"bit\", \"shape\", \"mse_before\", \"mse_after\", \"acc\"],\n",
        ")\n",
        "print_freq = 100\n",
        "evaluate = \"evaluate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAz_Jm_aLcuh",
        "outputId": "cf98c0e9-31bc-4e01-bf1c-91c2b446cb63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Optimize 0:features.0.0 for 8 bit of shape torch.Size([32, 3, 3, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 84.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.2649571895599365\n",
            "MSE after optimization:  0.029226889833807945\n",
            "cashed quant Inputfeatures.1.conv.0.0\n",
            "\n",
            "Optimize 1:features.1.conv.0.0 for 8 bit of shape torch.Size([32, 1, 3, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 45.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.002470217877998948\n",
            "MSE after optimization:  0.0009993225103244185\n",
            "cashed quant Inputfeatures.1.conv.1\n",
            "\n",
            "Optimize 2:features.1.conv.1 for 8 bit of shape torch.Size([16, 32, 1, 1])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 55.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.0028403126634657383\n",
            "MSE after optimization:  0.0027618911117315292\n",
            "cashed quant Inputfeatures.2.conv.0.0\n",
            "\n",
            "Optimize 3:features.2.conv.0.0 for 8 bit of shape torch.Size([96, 16, 1, 1])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 53.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.05337725952267647\n",
            "MSE after optimization:  0.05337467044591904\n",
            "cashed quant Inputfeatures.2.conv.1.0\n",
            "\n",
            "Optimize 4:features.2.conv.1.0 for 8 bit of shape torch.Size([96, 1, 3, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:04<00:00, 23.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.04495564475655556\n",
            "MSE after optimization:  0.03622474893927574\n",
            "cashed quant Inputfeatures.2.conv.2\n",
            "\n",
            "Optimize 5:features.2.conv.2 for 8 bit of shape torch.Size([24, 96, 1, 1])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 55.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.08768093585968018\n",
            "MSE after optimization:  0.08690611273050308\n",
            "cashed quant Inputfeatures.3.conv.0.0\n",
            "\n",
            "Optimize 6:features.3.conv.0.0 for 8 bit of shape torch.Size([144, 24, 1, 1])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 100.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE before optimization: 0.08365686237812042\n",
            "MSE after optimization:  0.07724959403276443\n",
            "cashed quant Inputfeatures.3.conv.1.0\n",
            "\n",
            "Optimize 7:features.3.conv.1.0 for 8 bit of shape torch.Size([144, 1, 3, 3])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 45/100 [00:01<00:01, 38.62it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[53], line 24\u001b[0m\n\u001b[0;32m     17\u001b[0m     handler\u001b[39m.\u001b[39mremove()\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOptimize \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m bit of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     20\u001b[0m         i, layer\u001b[39m.\u001b[39mname, layer\u001b[39m.\u001b[39mnum_bits, layer\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape\n\u001b[0;32m     21\u001b[0m     )\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m mse_before, mse_after \u001b[39m=\u001b[39m optimize_layer_adaquant(layer, cached_input_output[layer])\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMSE before optimization: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(mse_before))\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMSE after optimization:  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(mse_after))\n",
            "File \u001b[1;32md:\\aga\\MO\\Optimization_Adaquant\\utils\\adaquant.py:64\u001b[0m, in \u001b[0;36moptimize_layer_adaquant\u001b[1;34m(layer, in_out, test_batch_size, train_batch_size, iters, progress)\u001b[0m\n\u001b[0;32m     61\u001b[0m opt_qparams_w\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     63\u001b[0m \u001b[39m### BACKPROPAGATION\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     66\u001b[0m \u001b[39m### OPTIMIZERS STEP\u001b[39;00m\n\u001b[0;32m     67\u001b[0m opt_w\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\pwkpi\\miniconda3\\envs\\env_inz_test\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\pwkpi\\miniconda3\\envs\\env_inz_test\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\pwkpi\\miniconda3\\envs\\env_inz_test\\lib\\site-packages\\torch\\autograd\\function.py:257\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 257\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m    258\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    259\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i, layer in enumerate(cached_input_output):\n",
        "    if i > 0:  # and seq_adaquant = True\n",
        "        count = 0\n",
        "        cached_qinput = {}\n",
        "        for name, m in model.named_modules():\n",
        "            if layer.name == name:\n",
        "                if count < 5:\n",
        "                    handler = m.register_forward_hook(partial(Qhook, name))\n",
        "                    count += 1\n",
        "        # Store input/output for all quantizable layers\n",
        "        trainer.validate(calib_data)\n",
        "        print(\"cashed quant Input%s\" % layer.name)\n",
        "        cached_input_output[layer][0] = (\n",
        "            cached_qinput[layer][0],\n",
        "            cached_input_output[layer][0][1],\n",
        "        )\n",
        "        handler.remove()\n",
        "    print(\n",
        "        \"\\nOptimize {}:{} for {} bit of shape {}\".format(\n",
        "            i, layer.name, layer.num_bits, layer.weight.shape\n",
        "        )\n",
        "    )\n",
        "\n",
        "    mse_before, mse_after = optimize_layer_adaquant(layer, cached_input_output[layer])\n",
        "    acc=val_loop(val_dataloader, model, 'cpu')\n",
        "\n",
        "    print(\"\\nMSE before optimization: {}\".format(mse_before))\n",
        "    print(\"MSE after optimization:  {}\".format(mse_after))\n",
        "    mse_df.loc[i, \"name\"] = layer.name\n",
        "    mse_df.loc[i, \"bit\"] = layer.num_bits\n",
        "    mse_df.loc[i, \"shape\"] = str(layer.weight.shape)\n",
        "    mse_df.loc[i, \"mse_before\"] = mse_before\n",
        "    mse_df.loc[i, \"mse_after\"] = mse_after\n",
        "    mse_df.loc[i, \"acc\"] = acc\n",
        "\n",
        "\n",
        "mse_csv = evaluate + \".mse.csv\"\n",
        "mse_df.to_csv(mse_csv)\n",
        "\n",
        "filename = evaluate + \"_adaquant_val02\"\n",
        "torch.save(model.state_dict(), filename)\n",
        "\n",
        "calib_data = None\n",
        "cached_input_output = None\n",
        "# val_results = trainer.validate(val_data.get_loader())\n",
        "# logging.info(val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = evaluate + \"_adaquant.pth\"\n",
        "torch.save(model.state_dict(), filename)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mobilenet experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_predicted, all_labels = predict_from_dataloader(calib_data, model)\n",
        "acc = accuracy_score(all_labels.cpu().numpy(), all_predicted.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_xd = MobileNetV2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_predicted, all_labels = predict_from_dataloader(calib_data, model_xd)\n",
        "acc_xd = accuracy_score(all_labels.cpu().numpy(), all_predicted.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.071"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_xd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['features.0.0.equ_scale', 'features.0.0.quantize_input.running_zero_point', 'features.0.0.quantize_input.running_range', 'features.0.0.quantize_weight.running_zero_point', 'features.0.0.quantize_weight.running_range', 'features.1.conv.0.0.equ_scale', 'features.1.conv.0.0.quantize_input.running_zero_point', 'features.1.conv.0.0.quantize_input.running_range', 'features.1.conv.0.0.quantize_weight.running_zero_point', 'features.1.conv.0.0.quantize_weight.running_range', 'features.1.conv.1.equ_scale', 'features.1.conv.1.quantize_input.running_zero_point', 'features.1.conv.1.quantize_input.running_range', 'features.1.conv.1.quantize_weight.running_zero_point', 'features.1.conv.1.quantize_weight.running_range', 'features.2.conv.0.0.equ_scale', 'features.2.conv.0.0.quantize_input.running_zero_point', 'features.2.conv.0.0.quantize_input.running_range', 'features.2.conv.0.0.quantize_weight.running_zero_point', 'features.2.conv.0.0.quantize_weight.running_range', 'features.2.conv.1.0.equ_scale', 'features.2.conv.1.0.quantize_input.running_zero_point', 'features.2.conv.1.0.quantize_input.running_range', 'features.2.conv.1.0.quantize_weight.running_zero_point', 'features.2.conv.1.0.quantize_weight.running_range', 'features.2.conv.2.equ_scale', 'features.2.conv.2.quantize_input.running_zero_point', 'features.2.conv.2.quantize_input.running_range', 'features.2.conv.2.quantize_weight.running_zero_point', 'features.2.conv.2.quantize_weight.running_range', 'features.3.conv.0.0.equ_scale', 'features.3.conv.0.0.quantize_input.running_zero_point', 'features.3.conv.0.0.quantize_input.running_range', 'features.3.conv.0.0.quantize_weight.running_zero_point', 'features.3.conv.0.0.quantize_weight.running_range', 'features.3.conv.1.0.equ_scale', 'features.3.conv.1.0.quantize_input.running_zero_point', 'features.3.conv.1.0.quantize_input.running_range', 'features.3.conv.1.0.quantize_weight.running_zero_point', 'features.3.conv.1.0.quantize_weight.running_range', 'features.3.conv.2.equ_scale', 'features.3.conv.2.quantize_input.running_zero_point', 'features.3.conv.2.quantize_input.running_range', 'features.3.conv.2.quantize_weight.running_zero_point', 'features.3.conv.2.quantize_weight.running_range', 'features.4.conv.0.0.equ_scale', 'features.4.conv.0.0.quantize_input.running_zero_point', 'features.4.conv.0.0.quantize_input.running_range', 'features.4.conv.0.0.quantize_weight.running_zero_point', 'features.4.conv.0.0.quantize_weight.running_range', 'features.4.conv.1.0.equ_scale', 'features.4.conv.1.0.quantize_input.running_zero_point', 'features.4.conv.1.0.quantize_input.running_range', 'features.4.conv.1.0.quantize_weight.running_zero_point', 'features.4.conv.1.0.quantize_weight.running_range', 'features.4.conv.2.equ_scale', 'features.4.conv.2.quantize_input.running_zero_point', 'features.4.conv.2.quantize_input.running_range', 'features.4.conv.2.quantize_weight.running_zero_point', 'features.4.conv.2.quantize_weight.running_range', 'features.5.conv.0.0.equ_scale', 'features.5.conv.0.0.quantize_input.running_zero_point', 'features.5.conv.0.0.quantize_input.running_range', 'features.5.conv.0.0.quantize_weight.running_zero_point', 'features.5.conv.0.0.quantize_weight.running_range', 'features.5.conv.1.0.equ_scale', 'features.5.conv.1.0.quantize_input.running_zero_point', 'features.5.conv.1.0.quantize_input.running_range', 'features.5.conv.1.0.quantize_weight.running_zero_point', 'features.5.conv.1.0.quantize_weight.running_range', 'features.5.conv.2.equ_scale', 'features.5.conv.2.quantize_input.running_zero_point', 'features.5.conv.2.quantize_input.running_range', 'features.5.conv.2.quantize_weight.running_zero_point', 'features.5.conv.2.quantize_weight.running_range', 'features.6.conv.0.0.equ_scale', 'features.6.conv.0.0.quantize_input.running_zero_point', 'features.6.conv.0.0.quantize_input.running_range', 'features.6.conv.0.0.quantize_weight.running_zero_point', 'features.6.conv.0.0.quantize_weight.running_range', 'features.6.conv.1.0.equ_scale', 'features.6.conv.1.0.quantize_input.running_zero_point', 'features.6.conv.1.0.quantize_input.running_range', 'features.6.conv.1.0.quantize_weight.running_zero_point', 'features.6.conv.1.0.quantize_weight.running_range', 'features.6.conv.2.equ_scale', 'features.6.conv.2.quantize_input.running_zero_point', 'features.6.conv.2.quantize_input.running_range', 'features.6.conv.2.quantize_weight.running_zero_point', 'features.6.conv.2.quantize_weight.running_range', 'features.7.conv.0.0.equ_scale', 'features.7.conv.0.0.quantize_input.running_zero_point', 'features.7.conv.0.0.quantize_input.running_range', 'features.7.conv.0.0.quantize_weight.running_zero_point', 'features.7.conv.0.0.quantize_weight.running_range', 'features.7.conv.1.0.equ_scale', 'features.7.conv.1.0.quantize_input.running_zero_point', 'features.7.conv.1.0.quantize_input.running_range', 'features.7.conv.1.0.quantize_weight.running_zero_point', 'features.7.conv.1.0.quantize_weight.running_range', 'features.7.conv.2.equ_scale', 'features.7.conv.2.quantize_input.running_zero_point', 'features.7.conv.2.quantize_input.running_range', 'features.7.conv.2.quantize_weight.running_zero_point', 'features.7.conv.2.quantize_weight.running_range', 'features.8.conv.0.0.equ_scale', 'features.8.conv.0.0.quantize_input.running_zero_point', 'features.8.conv.0.0.quantize_input.running_range', 'features.8.conv.0.0.quantize_weight.running_zero_point', 'features.8.conv.0.0.quantize_weight.running_range', 'features.8.conv.1.0.equ_scale', 'features.8.conv.1.0.quantize_input.running_zero_point', 'features.8.conv.1.0.quantize_input.running_range', 'features.8.conv.1.0.quantize_weight.running_zero_point', 'features.8.conv.1.0.quantize_weight.running_range', 'features.8.conv.2.equ_scale', 'features.8.conv.2.quantize_input.running_zero_point', 'features.8.conv.2.quantize_input.running_range', 'features.8.conv.2.quantize_weight.running_zero_point', 'features.8.conv.2.quantize_weight.running_range', 'features.9.conv.0.0.equ_scale', 'features.9.conv.0.0.quantize_input.running_zero_point', 'features.9.conv.0.0.quantize_input.running_range', 'features.9.conv.0.0.quantize_weight.running_zero_point', 'features.9.conv.0.0.quantize_weight.running_range', 'features.9.conv.1.0.equ_scale', 'features.9.conv.1.0.quantize_input.running_zero_point', 'features.9.conv.1.0.quantize_input.running_range', 'features.9.conv.1.0.quantize_weight.running_zero_point', 'features.9.conv.1.0.quantize_weight.running_range', 'features.9.conv.2.equ_scale', 'features.9.conv.2.quantize_input.running_zero_point', 'features.9.conv.2.quantize_input.running_range', 'features.9.conv.2.quantize_weight.running_zero_point', 'features.9.conv.2.quantize_weight.running_range', 'features.10.conv.0.0.equ_scale', 'features.10.conv.0.0.quantize_input.running_zero_point', 'features.10.conv.0.0.quantize_input.running_range', 'features.10.conv.0.0.quantize_weight.running_zero_point', 'features.10.conv.0.0.quantize_weight.running_range', 'features.10.conv.1.0.equ_scale', 'features.10.conv.1.0.quantize_input.running_zero_point', 'features.10.conv.1.0.quantize_input.running_range', 'features.10.conv.1.0.quantize_weight.running_zero_point', 'features.10.conv.1.0.quantize_weight.running_range', 'features.10.conv.2.equ_scale', 'features.10.conv.2.quantize_input.running_zero_point', 'features.10.conv.2.quantize_input.running_range', 'features.10.conv.2.quantize_weight.running_zero_point', 'features.10.conv.2.quantize_weight.running_range', 'features.11.conv.0.0.equ_scale', 'features.11.conv.0.0.quantize_input.running_zero_point', 'features.11.conv.0.0.quantize_input.running_range', 'features.11.conv.0.0.quantize_weight.running_zero_point', 'features.11.conv.0.0.quantize_weight.running_range', 'features.11.conv.1.0.equ_scale', 'features.11.conv.1.0.quantize_input.running_zero_point', 'features.11.conv.1.0.quantize_input.running_range', 'features.11.conv.1.0.quantize_weight.running_zero_point', 'features.11.conv.1.0.quantize_weight.running_range', 'features.11.conv.2.equ_scale', 'features.11.conv.2.quantize_input.running_zero_point', 'features.11.conv.2.quantize_input.running_range', 'features.11.conv.2.quantize_weight.running_zero_point', 'features.11.conv.2.quantize_weight.running_range', 'features.12.conv.0.0.equ_scale', 'features.12.conv.0.0.quantize_input.running_zero_point', 'features.12.conv.0.0.quantize_input.running_range', 'features.12.conv.0.0.quantize_weight.running_zero_point', 'features.12.conv.0.0.quantize_weight.running_range', 'features.12.conv.1.0.equ_scale', 'features.12.conv.1.0.quantize_input.running_zero_point', 'features.12.conv.1.0.quantize_input.running_range', 'features.12.conv.1.0.quantize_weight.running_zero_point', 'features.12.conv.1.0.quantize_weight.running_range', 'features.12.conv.2.equ_scale', 'features.12.conv.2.quantize_input.running_zero_point', 'features.12.conv.2.quantize_input.running_range', 'features.12.conv.2.quantize_weight.running_zero_point', 'features.12.conv.2.quantize_weight.running_range', 'features.13.conv.0.0.equ_scale', 'features.13.conv.0.0.quantize_input.running_zero_point', 'features.13.conv.0.0.quantize_input.running_range', 'features.13.conv.0.0.quantize_weight.running_zero_point', 'features.13.conv.0.0.quantize_weight.running_range', 'features.13.conv.1.0.equ_scale', 'features.13.conv.1.0.quantize_input.running_zero_point', 'features.13.conv.1.0.quantize_input.running_range', 'features.13.conv.1.0.quantize_weight.running_zero_point', 'features.13.conv.1.0.quantize_weight.running_range', 'features.13.conv.2.equ_scale', 'features.13.conv.2.quantize_input.running_zero_point', 'features.13.conv.2.quantize_input.running_range', 'features.13.conv.2.quantize_weight.running_zero_point', 'features.13.conv.2.quantize_weight.running_range', 'features.14.conv.0.0.equ_scale', 'features.14.conv.0.0.quantize_input.running_zero_point', 'features.14.conv.0.0.quantize_input.running_range', 'features.14.conv.0.0.quantize_weight.running_zero_point', 'features.14.conv.0.0.quantize_weight.running_range', 'features.14.conv.1.0.equ_scale', 'features.14.conv.1.0.quantize_input.running_zero_point', 'features.14.conv.1.0.quantize_input.running_range', 'features.14.conv.1.0.quantize_weight.running_zero_point', 'features.14.conv.1.0.quantize_weight.running_range', 'features.14.conv.2.equ_scale', 'features.14.conv.2.quantize_input.running_zero_point', 'features.14.conv.2.quantize_input.running_range', 'features.14.conv.2.quantize_weight.running_zero_point', 'features.14.conv.2.quantize_weight.running_range', 'features.15.conv.0.0.equ_scale', 'features.15.conv.0.0.quantize_input.running_zero_point', 'features.15.conv.0.0.quantize_input.running_range', 'features.15.conv.0.0.quantize_weight.running_zero_point', 'features.15.conv.0.0.quantize_weight.running_range', 'features.15.conv.1.0.equ_scale', 'features.15.conv.1.0.quantize_input.running_zero_point', 'features.15.conv.1.0.quantize_input.running_range', 'features.15.conv.1.0.quantize_weight.running_zero_point', 'features.15.conv.1.0.quantize_weight.running_range', 'features.15.conv.2.equ_scale', 'features.15.conv.2.quantize_input.running_zero_point', 'features.15.conv.2.quantize_input.running_range', 'features.15.conv.2.quantize_weight.running_zero_point', 'features.15.conv.2.quantize_weight.running_range', 'features.16.conv.0.0.equ_scale', 'features.16.conv.0.0.quantize_input.running_zero_point', 'features.16.conv.0.0.quantize_input.running_range', 'features.16.conv.0.0.quantize_weight.running_zero_point', 'features.16.conv.0.0.quantize_weight.running_range', 'features.16.conv.1.0.equ_scale', 'features.16.conv.1.0.quantize_input.running_zero_point', 'features.16.conv.1.0.quantize_input.running_range', 'features.16.conv.1.0.quantize_weight.running_zero_point', 'features.16.conv.1.0.quantize_weight.running_range', 'features.16.conv.2.equ_scale', 'features.16.conv.2.quantize_input.running_zero_point', 'features.16.conv.2.quantize_input.running_range', 'features.16.conv.2.quantize_weight.running_zero_point', 'features.16.conv.2.quantize_weight.running_range', 'features.17.conv.0.0.equ_scale', 'features.17.conv.0.0.quantize_input.running_zero_point', 'features.17.conv.0.0.quantize_input.running_range', 'features.17.conv.0.0.quantize_weight.running_zero_point', 'features.17.conv.0.0.quantize_weight.running_range', 'features.17.conv.1.0.equ_scale', 'features.17.conv.1.0.quantize_input.running_zero_point', 'features.17.conv.1.0.quantize_input.running_range', 'features.17.conv.1.0.quantize_weight.running_zero_point', 'features.17.conv.1.0.quantize_weight.running_range', 'features.17.conv.2.equ_scale', 'features.17.conv.2.quantize_input.running_zero_point', 'features.17.conv.2.quantize_input.running_range', 'features.17.conv.2.quantize_weight.running_zero_point', 'features.17.conv.2.quantize_weight.running_range', 'features.18.0.equ_scale', 'features.18.0.quantize_input.running_zero_point', 'features.18.0.quantize_input.running_range', 'features.18.0.quantize_weight.running_zero_point', 'features.18.0.quantize_weight.running_range', 'classifier.1.equ_scale', 'classifier.1.quantize_input.running_zero_point', 'classifier.1.quantize_input.running_range', 'classifier.1.quantize_weight.running_zero_point', 'classifier.1.quantize_weight.running_range'], unexpected_keys=[])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_xd.load_state_dict(model.state_dict(), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_predicted, all_labels = predict_from_dataloader(calib_data, model_xd)\n",
        "acc_xd_after = accuracy_score(all_labels.cpu().numpy(), all_predicted.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_xd_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model_xd.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "213\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for name, m in model.named_modules():\n",
        "    i += 1\n",
        "    # print(name)\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        print(name)\n",
        "        # m.name = name\n",
        "        # # print(m.name)\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features.0.0\n",
            "features.1.conv.0.0\n",
            "features.1.conv.1\n",
            "features.2.conv.0.0\n",
            "features.2.conv.1.0\n",
            "features.2.conv.2\n",
            "features.3.conv.0.0\n",
            "features.3.conv.1.0\n",
            "features.3.conv.2\n",
            "features.4.conv.0.0\n",
            "features.4.conv.1.0\n",
            "features.4.conv.2\n",
            "features.5.conv.0.0\n",
            "features.5.conv.1.0\n",
            "features.5.conv.2\n",
            "features.6.conv.0.0\n",
            "features.6.conv.1.0\n",
            "features.6.conv.2\n",
            "features.7.conv.0.0\n",
            "features.7.conv.1.0\n",
            "features.7.conv.2\n",
            "features.8.conv.0.0\n",
            "features.8.conv.1.0\n",
            "features.8.conv.2\n",
            "features.9.conv.0.0\n",
            "features.9.conv.1.0\n",
            "features.9.conv.2\n",
            "features.10.conv.0.0\n",
            "features.10.conv.1.0\n",
            "features.10.conv.2\n",
            "features.11.conv.0.0\n",
            "features.11.conv.1.0\n",
            "features.11.conv.2\n",
            "features.12.conv.0.0\n",
            "features.12.conv.1.0\n",
            "features.12.conv.2\n",
            "features.13.conv.0.0\n",
            "features.13.conv.1.0\n",
            "features.13.conv.2\n",
            "features.14.conv.0.0\n",
            "features.14.conv.1.0\n",
            "features.14.conv.2\n",
            "features.15.conv.0.0\n",
            "features.15.conv.1.0\n",
            "features.15.conv.2\n",
            "features.16.conv.0.0\n",
            "features.16.conv.1.0\n",
            "features.16.conv.2\n",
            "features.17.conv.0.0\n",
            "features.17.conv.1.0\n",
            "features.17.conv.2\n",
            "features.18.0\n",
            "classifier.1\n",
            "319\n"
          ]
        }
      ],
      "source": [
        "j = 0\n",
        "for name, m in model_xd.named_modules():\n",
        "    # print(name)\n",
        "    j += 1\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        print(name)\n",
        "    #     m.name = name\n",
        "    #     # print(m.name)\n",
        "print(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.features[1].conv[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuantThUpdate()"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_xd.features[1].conv[0][0].quantize_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features.0.0.weight torch.Size([32, 3, 3, 3])\n",
            "features.0.1.weight torch.Size([32])\n",
            "features.0.1.bias torch.Size([32])\n",
            "features.1.conv.0.0.weight torch.Size([32, 1, 3, 3])\n",
            "features.1.conv.0.1.weight torch.Size([32])\n",
            "features.1.conv.0.1.bias torch.Size([32])\n",
            "features.1.conv.1.weight torch.Size([16, 32, 1, 1])\n",
            "features.1.conv.2.weight torch.Size([16])\n",
            "features.1.conv.2.bias torch.Size([16])\n",
            "features.2.conv.0.0.weight torch.Size([96, 16, 1, 1])\n",
            "features.2.conv.0.1.weight torch.Size([96])\n",
            "features.2.conv.0.1.bias torch.Size([96])\n",
            "features.2.conv.1.0.weight torch.Size([96, 1, 3, 3])\n",
            "features.2.conv.1.1.weight torch.Size([96])\n",
            "features.2.conv.1.1.bias torch.Size([96])\n",
            "features.2.conv.2.weight torch.Size([24, 96, 1, 1])\n",
            "features.2.conv.3.weight torch.Size([24])\n",
            "features.2.conv.3.bias torch.Size([24])\n",
            "features.3.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
            "features.3.conv.0.1.weight torch.Size([144])\n",
            "features.3.conv.0.1.bias torch.Size([144])\n",
            "features.3.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
            "features.3.conv.1.1.weight torch.Size([144])\n",
            "features.3.conv.1.1.bias torch.Size([144])\n",
            "features.3.conv.2.weight torch.Size([24, 144, 1, 1])\n",
            "features.3.conv.3.weight torch.Size([24])\n",
            "features.3.conv.3.bias torch.Size([24])\n",
            "features.4.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
            "features.4.conv.0.1.weight torch.Size([144])\n",
            "features.4.conv.0.1.bias torch.Size([144])\n",
            "features.4.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
            "features.4.conv.1.1.weight torch.Size([144])\n",
            "features.4.conv.1.1.bias torch.Size([144])\n",
            "features.4.conv.2.weight torch.Size([32, 144, 1, 1])\n",
            "features.4.conv.3.weight torch.Size([32])\n",
            "features.4.conv.3.bias torch.Size([32])\n",
            "features.5.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
            "features.5.conv.0.1.weight torch.Size([192])\n",
            "features.5.conv.0.1.bias torch.Size([192])\n",
            "features.5.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
            "features.5.conv.1.1.weight torch.Size([192])\n",
            "features.5.conv.1.1.bias torch.Size([192])\n",
            "features.5.conv.2.weight torch.Size([32, 192, 1, 1])\n",
            "features.5.conv.3.weight torch.Size([32])\n",
            "features.5.conv.3.bias torch.Size([32])\n",
            "features.6.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
            "features.6.conv.0.1.weight torch.Size([192])\n",
            "features.6.conv.0.1.bias torch.Size([192])\n",
            "features.6.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
            "features.6.conv.1.1.weight torch.Size([192])\n",
            "features.6.conv.1.1.bias torch.Size([192])\n",
            "features.6.conv.2.weight torch.Size([32, 192, 1, 1])\n",
            "features.6.conv.3.weight torch.Size([32])\n",
            "features.6.conv.3.bias torch.Size([32])\n",
            "features.7.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
            "features.7.conv.0.1.weight torch.Size([192])\n",
            "features.7.conv.0.1.bias torch.Size([192])\n",
            "features.7.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
            "features.7.conv.1.1.weight torch.Size([192])\n",
            "features.7.conv.1.1.bias torch.Size([192])\n",
            "features.7.conv.2.weight torch.Size([64, 192, 1, 1])\n",
            "features.7.conv.3.weight torch.Size([64])\n",
            "features.7.conv.3.bias torch.Size([64])\n",
            "features.8.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.8.conv.0.1.weight torch.Size([384])\n",
            "features.8.conv.0.1.bias torch.Size([384])\n",
            "features.8.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.8.conv.1.1.weight torch.Size([384])\n",
            "features.8.conv.1.1.bias torch.Size([384])\n",
            "features.8.conv.2.weight torch.Size([64, 384, 1, 1])\n",
            "features.8.conv.3.weight torch.Size([64])\n",
            "features.8.conv.3.bias torch.Size([64])\n",
            "features.9.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.9.conv.0.1.weight torch.Size([384])\n",
            "features.9.conv.0.1.bias torch.Size([384])\n",
            "features.9.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.9.conv.1.1.weight torch.Size([384])\n",
            "features.9.conv.1.1.bias torch.Size([384])\n",
            "features.9.conv.2.weight torch.Size([64, 384, 1, 1])\n",
            "features.9.conv.3.weight torch.Size([64])\n",
            "features.9.conv.3.bias torch.Size([64])\n",
            "features.10.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.10.conv.0.1.weight torch.Size([384])\n",
            "features.10.conv.0.1.bias torch.Size([384])\n",
            "features.10.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.10.conv.1.1.weight torch.Size([384])\n",
            "features.10.conv.1.1.bias torch.Size([384])\n",
            "features.10.conv.2.weight torch.Size([64, 384, 1, 1])\n",
            "features.10.conv.3.weight torch.Size([64])\n",
            "features.10.conv.3.bias torch.Size([64])\n",
            "features.11.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.11.conv.0.1.weight torch.Size([384])\n",
            "features.11.conv.0.1.bias torch.Size([384])\n",
            "features.11.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.11.conv.1.1.weight torch.Size([384])\n",
            "features.11.conv.1.1.bias torch.Size([384])\n",
            "features.11.conv.2.weight torch.Size([96, 384, 1, 1])\n",
            "features.11.conv.3.weight torch.Size([96])\n",
            "features.11.conv.3.bias torch.Size([96])\n",
            "features.12.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
            "features.12.conv.0.1.weight torch.Size([576])\n",
            "features.12.conv.0.1.bias torch.Size([576])\n",
            "features.12.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
            "features.12.conv.1.1.weight torch.Size([576])\n",
            "features.12.conv.1.1.bias torch.Size([576])\n",
            "features.12.conv.2.weight torch.Size([96, 576, 1, 1])\n",
            "features.12.conv.3.weight torch.Size([96])\n",
            "features.12.conv.3.bias torch.Size([96])\n",
            "features.13.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
            "features.13.conv.0.1.weight torch.Size([576])\n",
            "features.13.conv.0.1.bias torch.Size([576])\n",
            "features.13.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
            "features.13.conv.1.1.weight torch.Size([576])\n",
            "features.13.conv.1.1.bias torch.Size([576])\n",
            "features.13.conv.2.weight torch.Size([96, 576, 1, 1])\n",
            "features.13.conv.3.weight torch.Size([96])\n",
            "features.13.conv.3.bias torch.Size([96])\n",
            "features.14.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
            "features.14.conv.0.1.weight torch.Size([576])\n",
            "features.14.conv.0.1.bias torch.Size([576])\n",
            "features.14.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
            "features.14.conv.1.1.weight torch.Size([576])\n",
            "features.14.conv.1.1.bias torch.Size([576])\n",
            "features.14.conv.2.weight torch.Size([160, 576, 1, 1])\n",
            "features.14.conv.3.weight torch.Size([160])\n",
            "features.14.conv.3.bias torch.Size([160])\n",
            "features.15.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
            "features.15.conv.0.1.weight torch.Size([960])\n",
            "features.15.conv.0.1.bias torch.Size([960])\n",
            "features.15.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
            "features.15.conv.1.1.weight torch.Size([960])\n",
            "features.15.conv.1.1.bias torch.Size([960])\n",
            "features.15.conv.2.weight torch.Size([160, 960, 1, 1])\n",
            "features.15.conv.3.weight torch.Size([160])\n",
            "features.15.conv.3.bias torch.Size([160])\n",
            "features.16.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
            "features.16.conv.0.1.weight torch.Size([960])\n",
            "features.16.conv.0.1.bias torch.Size([960])\n",
            "features.16.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
            "features.16.conv.1.1.weight torch.Size([960])\n",
            "features.16.conv.1.1.bias torch.Size([960])\n",
            "features.16.conv.2.weight torch.Size([160, 960, 1, 1])\n",
            "features.16.conv.3.weight torch.Size([160])\n",
            "features.16.conv.3.bias torch.Size([160])\n",
            "features.17.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
            "features.17.conv.0.1.weight torch.Size([960])\n",
            "features.17.conv.0.1.bias torch.Size([960])\n",
            "features.17.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
            "features.17.conv.1.1.weight torch.Size([960])\n",
            "features.17.conv.1.1.bias torch.Size([960])\n",
            "features.17.conv.2.weight torch.Size([320, 960, 1, 1])\n",
            "features.17.conv.3.weight torch.Size([320])\n",
            "features.17.conv.3.bias torch.Size([320])\n",
            "features.18.0.weight torch.Size([1280, 320, 1, 1])\n",
            "features.18.1.weight torch.Size([1280])\n",
            "features.18.1.bias torch.Size([1280])\n",
            "classifier.1.weight torch.Size([10, 1280])\n",
            "classifier.1.bias torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features.0.0.weight torch.Size([32, 3, 3, 3])\n",
            "features.0.0.equ_scale torch.Size([32, 1, 1, 1])\n",
            "features.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.0.0.quantize_weight.running_zero_point torch.Size([32, 1, 1, 1])\n",
            "features.0.0.quantize_weight.running_range torch.Size([32, 1, 1, 1])\n",
            "features.0.1.weight torch.Size([32])\n",
            "features.0.1.bias torch.Size([32])\n",
            "features.1.conv.0.0.weight torch.Size([32, 1, 3, 3])\n",
            "features.1.conv.0.0.equ_scale torch.Size([32, 1, 1, 1])\n",
            "features.1.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.1.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.1.conv.0.0.quantize_weight.running_zero_point torch.Size([32, 1, 1, 1])\n",
            "features.1.conv.0.0.quantize_weight.running_range torch.Size([32, 1, 1, 1])\n",
            "features.1.conv.0.1.weight torch.Size([32])\n",
            "features.1.conv.0.1.bias torch.Size([32])\n",
            "features.1.conv.1.weight torch.Size([16, 32, 1, 1])\n",
            "features.1.conv.1.equ_scale torch.Size([16, 1, 1, 1])\n",
            "features.1.conv.1.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.1.conv.1.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.1.conv.1.quantize_weight.running_zero_point torch.Size([16, 1, 1, 1])\n",
            "features.1.conv.1.quantize_weight.running_range torch.Size([16, 1, 1, 1])\n",
            "features.1.conv.2.weight torch.Size([16])\n",
            "features.1.conv.2.bias torch.Size([16])\n",
            "features.2.conv.0.0.weight torch.Size([96, 16, 1, 1])\n",
            "features.2.conv.0.0.equ_scale torch.Size([96, 1, 1, 1])\n",
            "features.2.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.2.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.2.conv.0.0.quantize_weight.running_zero_point torch.Size([96, 1, 1, 1])\n",
            "features.2.conv.0.0.quantize_weight.running_range torch.Size([96, 1, 1, 1])\n",
            "features.2.conv.0.1.weight torch.Size([96])\n",
            "features.2.conv.0.1.bias torch.Size([96])\n",
            "features.2.conv.1.0.weight torch.Size([96, 1, 3, 3])\n",
            "features.2.conv.1.0.equ_scale torch.Size([96, 1, 1, 1])\n",
            "features.2.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.2.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.2.conv.1.0.quantize_weight.running_zero_point torch.Size([96, 1, 1, 1])\n",
            "features.2.conv.1.0.quantize_weight.running_range torch.Size([96, 1, 1, 1])\n",
            "features.2.conv.1.1.weight torch.Size([96])\n",
            "features.2.conv.1.1.bias torch.Size([96])\n",
            "features.2.conv.2.weight torch.Size([24, 96, 1, 1])\n",
            "features.2.conv.2.equ_scale torch.Size([24, 1, 1, 1])\n",
            "features.2.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.2.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.2.conv.2.quantize_weight.running_zero_point torch.Size([24, 1, 1, 1])\n",
            "features.2.conv.2.quantize_weight.running_range torch.Size([24, 1, 1, 1])\n",
            "features.2.conv.3.weight torch.Size([24])\n",
            "features.2.conv.3.bias torch.Size([24])\n",
            "features.3.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
            "features.3.conv.0.0.equ_scale torch.Size([144, 1, 1, 1])\n",
            "features.3.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.3.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.3.conv.0.0.quantize_weight.running_zero_point torch.Size([144, 1, 1, 1])\n",
            "features.3.conv.0.0.quantize_weight.running_range torch.Size([144, 1, 1, 1])\n",
            "features.3.conv.0.1.weight torch.Size([144])\n",
            "features.3.conv.0.1.bias torch.Size([144])\n",
            "features.3.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
            "features.3.conv.1.0.equ_scale torch.Size([144, 1, 1, 1])\n",
            "features.3.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.3.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.3.conv.1.0.quantize_weight.running_zero_point torch.Size([144, 1, 1, 1])\n",
            "features.3.conv.1.0.quantize_weight.running_range torch.Size([144, 1, 1, 1])\n",
            "features.3.conv.1.1.weight torch.Size([144])\n",
            "features.3.conv.1.1.bias torch.Size([144])\n",
            "features.3.conv.2.weight torch.Size([24, 144, 1, 1])\n",
            "features.3.conv.2.equ_scale torch.Size([24, 1, 1, 1])\n",
            "features.3.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.3.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.3.conv.2.quantize_weight.running_zero_point torch.Size([24, 1, 1, 1])\n",
            "features.3.conv.2.quantize_weight.running_range torch.Size([24, 1, 1, 1])\n",
            "features.3.conv.3.weight torch.Size([24])\n",
            "features.3.conv.3.bias torch.Size([24])\n",
            "features.4.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
            "features.4.conv.0.0.equ_scale torch.Size([144, 1, 1, 1])\n",
            "features.4.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.4.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.4.conv.0.0.quantize_weight.running_zero_point torch.Size([144, 1, 1, 1])\n",
            "features.4.conv.0.0.quantize_weight.running_range torch.Size([144, 1, 1, 1])\n",
            "features.4.conv.0.1.weight torch.Size([144])\n",
            "features.4.conv.0.1.bias torch.Size([144])\n",
            "features.4.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
            "features.4.conv.1.0.equ_scale torch.Size([144, 1, 1, 1])\n",
            "features.4.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.4.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.4.conv.1.0.quantize_weight.running_zero_point torch.Size([144, 1, 1, 1])\n",
            "features.4.conv.1.0.quantize_weight.running_range torch.Size([144, 1, 1, 1])\n",
            "features.4.conv.1.1.weight torch.Size([144])\n",
            "features.4.conv.1.1.bias torch.Size([144])\n",
            "features.4.conv.2.weight torch.Size([32, 144, 1, 1])\n",
            "features.4.conv.2.equ_scale torch.Size([32, 1, 1, 1])\n",
            "features.4.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.4.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.4.conv.2.quantize_weight.running_zero_point torch.Size([32, 1, 1, 1])\n",
            "features.4.conv.2.quantize_weight.running_range torch.Size([32, 1, 1, 1])\n",
            "features.4.conv.3.weight torch.Size([32])\n",
            "features.4.conv.3.bias torch.Size([32])\n",
            "features.5.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
            "features.5.conv.0.0.equ_scale torch.Size([192, 1, 1, 1])\n",
            "features.5.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.5.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.5.conv.0.0.quantize_weight.running_zero_point torch.Size([192, 1, 1, 1])\n",
            "features.5.conv.0.0.quantize_weight.running_range torch.Size([192, 1, 1, 1])\n",
            "features.5.conv.0.1.weight torch.Size([192])\n",
            "features.5.conv.0.1.bias torch.Size([192])\n",
            "features.5.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
            "features.5.conv.1.0.equ_scale torch.Size([192, 1, 1, 1])\n",
            "features.5.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.5.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.5.conv.1.0.quantize_weight.running_zero_point torch.Size([192, 1, 1, 1])\n",
            "features.5.conv.1.0.quantize_weight.running_range torch.Size([192, 1, 1, 1])\n",
            "features.5.conv.1.1.weight torch.Size([192])\n",
            "features.5.conv.1.1.bias torch.Size([192])\n",
            "features.5.conv.2.weight torch.Size([32, 192, 1, 1])\n",
            "features.5.conv.2.equ_scale torch.Size([32, 1, 1, 1])\n",
            "features.5.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.5.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.5.conv.2.quantize_weight.running_zero_point torch.Size([32, 1, 1, 1])\n",
            "features.5.conv.2.quantize_weight.running_range torch.Size([32, 1, 1, 1])\n",
            "features.5.conv.3.weight torch.Size([32])\n",
            "features.5.conv.3.bias torch.Size([32])\n",
            "features.6.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
            "features.6.conv.0.0.equ_scale torch.Size([192, 1, 1, 1])\n",
            "features.6.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.6.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.6.conv.0.0.quantize_weight.running_zero_point torch.Size([192, 1, 1, 1])\n",
            "features.6.conv.0.0.quantize_weight.running_range torch.Size([192, 1, 1, 1])\n",
            "features.6.conv.0.1.weight torch.Size([192])\n",
            "features.6.conv.0.1.bias torch.Size([192])\n",
            "features.6.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
            "features.6.conv.1.0.equ_scale torch.Size([192, 1, 1, 1])\n",
            "features.6.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.6.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.6.conv.1.0.quantize_weight.running_zero_point torch.Size([192, 1, 1, 1])\n",
            "features.6.conv.1.0.quantize_weight.running_range torch.Size([192, 1, 1, 1])\n",
            "features.6.conv.1.1.weight torch.Size([192])\n",
            "features.6.conv.1.1.bias torch.Size([192])\n",
            "features.6.conv.2.weight torch.Size([32, 192, 1, 1])\n",
            "features.6.conv.2.equ_scale torch.Size([32, 1, 1, 1])\n",
            "features.6.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.6.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.6.conv.2.quantize_weight.running_zero_point torch.Size([32, 1, 1, 1])\n",
            "features.6.conv.2.quantize_weight.running_range torch.Size([32, 1, 1, 1])\n",
            "features.6.conv.3.weight torch.Size([32])\n",
            "features.6.conv.3.bias torch.Size([32])\n",
            "features.7.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
            "features.7.conv.0.0.equ_scale torch.Size([192, 1, 1, 1])\n",
            "features.7.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.7.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.7.conv.0.0.quantize_weight.running_zero_point torch.Size([192, 1, 1, 1])\n",
            "features.7.conv.0.0.quantize_weight.running_range torch.Size([192, 1, 1, 1])\n",
            "features.7.conv.0.1.weight torch.Size([192])\n",
            "features.7.conv.0.1.bias torch.Size([192])\n",
            "features.7.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
            "features.7.conv.1.0.equ_scale torch.Size([192, 1, 1, 1])\n",
            "features.7.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.7.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.7.conv.1.0.quantize_weight.running_zero_point torch.Size([192, 1, 1, 1])\n",
            "features.7.conv.1.0.quantize_weight.running_range torch.Size([192, 1, 1, 1])\n",
            "features.7.conv.1.1.weight torch.Size([192])\n",
            "features.7.conv.1.1.bias torch.Size([192])\n",
            "features.7.conv.2.weight torch.Size([64, 192, 1, 1])\n",
            "features.7.conv.2.equ_scale torch.Size([64, 1, 1, 1])\n",
            "features.7.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.7.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.7.conv.2.quantize_weight.running_zero_point torch.Size([64, 1, 1, 1])\n",
            "features.7.conv.2.quantize_weight.running_range torch.Size([64, 1, 1, 1])\n",
            "features.7.conv.3.weight torch.Size([64])\n",
            "features.7.conv.3.bias torch.Size([64])\n",
            "features.8.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.8.conv.0.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.8.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.8.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.8.conv.0.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.8.conv.0.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.8.conv.0.1.weight torch.Size([384])\n",
            "features.8.conv.0.1.bias torch.Size([384])\n",
            "features.8.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.8.conv.1.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.8.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.8.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.8.conv.1.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.8.conv.1.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.8.conv.1.1.weight torch.Size([384])\n",
            "features.8.conv.1.1.bias torch.Size([384])\n",
            "features.8.conv.2.weight torch.Size([64, 384, 1, 1])\n",
            "features.8.conv.2.equ_scale torch.Size([64, 1, 1, 1])\n",
            "features.8.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.8.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.8.conv.2.quantize_weight.running_zero_point torch.Size([64, 1, 1, 1])\n",
            "features.8.conv.2.quantize_weight.running_range torch.Size([64, 1, 1, 1])\n",
            "features.8.conv.3.weight torch.Size([64])\n",
            "features.8.conv.3.bias torch.Size([64])\n",
            "features.9.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.9.conv.0.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.9.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.9.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.9.conv.0.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.9.conv.0.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.9.conv.0.1.weight torch.Size([384])\n",
            "features.9.conv.0.1.bias torch.Size([384])\n",
            "features.9.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.9.conv.1.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.9.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.9.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.9.conv.1.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.9.conv.1.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.9.conv.1.1.weight torch.Size([384])\n",
            "features.9.conv.1.1.bias torch.Size([384])\n",
            "features.9.conv.2.weight torch.Size([64, 384, 1, 1])\n",
            "features.9.conv.2.equ_scale torch.Size([64, 1, 1, 1])\n",
            "features.9.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.9.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.9.conv.2.quantize_weight.running_zero_point torch.Size([64, 1, 1, 1])\n",
            "features.9.conv.2.quantize_weight.running_range torch.Size([64, 1, 1, 1])\n",
            "features.9.conv.3.weight torch.Size([64])\n",
            "features.9.conv.3.bias torch.Size([64])\n",
            "features.10.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.10.conv.0.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.10.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.10.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.10.conv.0.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.10.conv.0.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.10.conv.0.1.weight torch.Size([384])\n",
            "features.10.conv.0.1.bias torch.Size([384])\n",
            "features.10.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.10.conv.1.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.10.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.10.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.10.conv.1.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.10.conv.1.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.10.conv.1.1.weight torch.Size([384])\n",
            "features.10.conv.1.1.bias torch.Size([384])\n",
            "features.10.conv.2.weight torch.Size([64, 384, 1, 1])\n",
            "features.10.conv.2.equ_scale torch.Size([64, 1, 1, 1])\n",
            "features.10.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.10.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.10.conv.2.quantize_weight.running_zero_point torch.Size([64, 1, 1, 1])\n",
            "features.10.conv.2.quantize_weight.running_range torch.Size([64, 1, 1, 1])\n",
            "features.10.conv.3.weight torch.Size([64])\n",
            "features.10.conv.3.bias torch.Size([64])\n",
            "features.11.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
            "features.11.conv.0.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.11.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.11.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.11.conv.0.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.11.conv.0.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.11.conv.0.1.weight torch.Size([384])\n",
            "features.11.conv.0.1.bias torch.Size([384])\n",
            "features.11.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
            "features.11.conv.1.0.equ_scale torch.Size([384, 1, 1, 1])\n",
            "features.11.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.11.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.11.conv.1.0.quantize_weight.running_zero_point torch.Size([384, 1, 1, 1])\n",
            "features.11.conv.1.0.quantize_weight.running_range torch.Size([384, 1, 1, 1])\n",
            "features.11.conv.1.1.weight torch.Size([384])\n",
            "features.11.conv.1.1.bias torch.Size([384])\n",
            "features.11.conv.2.weight torch.Size([96, 384, 1, 1])\n",
            "features.11.conv.2.equ_scale torch.Size([96, 1, 1, 1])\n",
            "features.11.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.11.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.11.conv.2.quantize_weight.running_zero_point torch.Size([96, 1, 1, 1])\n",
            "features.11.conv.2.quantize_weight.running_range torch.Size([96, 1, 1, 1])\n",
            "features.11.conv.3.weight torch.Size([96])\n",
            "features.11.conv.3.bias torch.Size([96])\n",
            "features.12.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
            "features.12.conv.0.0.equ_scale torch.Size([576, 1, 1, 1])\n",
            "features.12.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.12.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.12.conv.0.0.quantize_weight.running_zero_point torch.Size([576, 1, 1, 1])\n",
            "features.12.conv.0.0.quantize_weight.running_range torch.Size([576, 1, 1, 1])\n",
            "features.12.conv.0.1.weight torch.Size([576])\n",
            "features.12.conv.0.1.bias torch.Size([576])\n",
            "features.12.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
            "features.12.conv.1.0.equ_scale torch.Size([576, 1, 1, 1])\n",
            "features.12.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.12.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.12.conv.1.0.quantize_weight.running_zero_point torch.Size([576, 1, 1, 1])\n",
            "features.12.conv.1.0.quantize_weight.running_range torch.Size([576, 1, 1, 1])\n",
            "features.12.conv.1.1.weight torch.Size([576])\n",
            "features.12.conv.1.1.bias torch.Size([576])\n",
            "features.12.conv.2.weight torch.Size([96, 576, 1, 1])\n",
            "features.12.conv.2.equ_scale torch.Size([96, 1, 1, 1])\n",
            "features.12.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.12.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.12.conv.2.quantize_weight.running_zero_point torch.Size([96, 1, 1, 1])\n",
            "features.12.conv.2.quantize_weight.running_range torch.Size([96, 1, 1, 1])\n",
            "features.12.conv.3.weight torch.Size([96])\n",
            "features.12.conv.3.bias torch.Size([96])\n",
            "features.13.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
            "features.13.conv.0.0.equ_scale torch.Size([576, 1, 1, 1])\n",
            "features.13.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.13.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.13.conv.0.0.quantize_weight.running_zero_point torch.Size([576, 1, 1, 1])\n",
            "features.13.conv.0.0.quantize_weight.running_range torch.Size([576, 1, 1, 1])\n",
            "features.13.conv.0.1.weight torch.Size([576])\n",
            "features.13.conv.0.1.bias torch.Size([576])\n",
            "features.13.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
            "features.13.conv.1.0.equ_scale torch.Size([576, 1, 1, 1])\n",
            "features.13.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.13.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.13.conv.1.0.quantize_weight.running_zero_point torch.Size([576, 1, 1, 1])\n",
            "features.13.conv.1.0.quantize_weight.running_range torch.Size([576, 1, 1, 1])\n",
            "features.13.conv.1.1.weight torch.Size([576])\n",
            "features.13.conv.1.1.bias torch.Size([576])\n",
            "features.13.conv.2.weight torch.Size([96, 576, 1, 1])\n",
            "features.13.conv.2.equ_scale torch.Size([96, 1, 1, 1])\n",
            "features.13.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.13.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.13.conv.2.quantize_weight.running_zero_point torch.Size([96, 1, 1, 1])\n",
            "features.13.conv.2.quantize_weight.running_range torch.Size([96, 1, 1, 1])\n",
            "features.13.conv.3.weight torch.Size([96])\n",
            "features.13.conv.3.bias torch.Size([96])\n",
            "features.14.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
            "features.14.conv.0.0.equ_scale torch.Size([576, 1, 1, 1])\n",
            "features.14.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.14.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.14.conv.0.0.quantize_weight.running_zero_point torch.Size([576, 1, 1, 1])\n",
            "features.14.conv.0.0.quantize_weight.running_range torch.Size([576, 1, 1, 1])\n",
            "features.14.conv.0.1.weight torch.Size([576])\n",
            "features.14.conv.0.1.bias torch.Size([576])\n",
            "features.14.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
            "features.14.conv.1.0.equ_scale torch.Size([576, 1, 1, 1])\n",
            "features.14.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.14.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.14.conv.1.0.quantize_weight.running_zero_point torch.Size([576, 1, 1, 1])\n",
            "features.14.conv.1.0.quantize_weight.running_range torch.Size([576, 1, 1, 1])\n",
            "features.14.conv.1.1.weight torch.Size([576])\n",
            "features.14.conv.1.1.bias torch.Size([576])\n",
            "features.14.conv.2.weight torch.Size([160, 576, 1, 1])\n",
            "features.14.conv.2.equ_scale torch.Size([160, 1, 1, 1])\n",
            "features.14.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.14.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.14.conv.2.quantize_weight.running_zero_point torch.Size([160, 1, 1, 1])\n",
            "features.14.conv.2.quantize_weight.running_range torch.Size([160, 1, 1, 1])\n",
            "features.14.conv.3.weight torch.Size([160])\n",
            "features.14.conv.3.bias torch.Size([160])\n",
            "features.15.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
            "features.15.conv.0.0.equ_scale torch.Size([960, 1, 1, 1])\n",
            "features.15.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.15.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.15.conv.0.0.quantize_weight.running_zero_point torch.Size([960, 1, 1, 1])\n",
            "features.15.conv.0.0.quantize_weight.running_range torch.Size([960, 1, 1, 1])\n",
            "features.15.conv.0.1.weight torch.Size([960])\n",
            "features.15.conv.0.1.bias torch.Size([960])\n",
            "features.15.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
            "features.15.conv.1.0.equ_scale torch.Size([960, 1, 1, 1])\n",
            "features.15.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.15.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.15.conv.1.0.quantize_weight.running_zero_point torch.Size([960, 1, 1, 1])\n",
            "features.15.conv.1.0.quantize_weight.running_range torch.Size([960, 1, 1, 1])\n",
            "features.15.conv.1.1.weight torch.Size([960])\n",
            "features.15.conv.1.1.bias torch.Size([960])\n",
            "features.15.conv.2.weight torch.Size([160, 960, 1, 1])\n",
            "features.15.conv.2.equ_scale torch.Size([160, 1, 1, 1])\n",
            "features.15.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.15.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.15.conv.2.quantize_weight.running_zero_point torch.Size([160, 1, 1, 1])\n",
            "features.15.conv.2.quantize_weight.running_range torch.Size([160, 1, 1, 1])\n",
            "features.15.conv.3.weight torch.Size([160])\n",
            "features.15.conv.3.bias torch.Size([160])\n",
            "features.16.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
            "features.16.conv.0.0.equ_scale torch.Size([960, 1, 1, 1])\n",
            "features.16.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.16.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.16.conv.0.0.quantize_weight.running_zero_point torch.Size([960, 1, 1, 1])\n",
            "features.16.conv.0.0.quantize_weight.running_range torch.Size([960, 1, 1, 1])\n",
            "features.16.conv.0.1.weight torch.Size([960])\n",
            "features.16.conv.0.1.bias torch.Size([960])\n",
            "features.16.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
            "features.16.conv.1.0.equ_scale torch.Size([960, 1, 1, 1])\n",
            "features.16.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.16.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.16.conv.1.0.quantize_weight.running_zero_point torch.Size([960, 1, 1, 1])\n",
            "features.16.conv.1.0.quantize_weight.running_range torch.Size([960, 1, 1, 1])\n",
            "features.16.conv.1.1.weight torch.Size([960])\n",
            "features.16.conv.1.1.bias torch.Size([960])\n",
            "features.16.conv.2.weight torch.Size([160, 960, 1, 1])\n",
            "features.16.conv.2.equ_scale torch.Size([160, 1, 1, 1])\n",
            "features.16.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.16.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.16.conv.2.quantize_weight.running_zero_point torch.Size([160, 1, 1, 1])\n",
            "features.16.conv.2.quantize_weight.running_range torch.Size([160, 1, 1, 1])\n",
            "features.16.conv.3.weight torch.Size([160])\n",
            "features.16.conv.3.bias torch.Size([160])\n",
            "features.17.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
            "features.17.conv.0.0.equ_scale torch.Size([960, 1, 1, 1])\n",
            "features.17.conv.0.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.17.conv.0.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.17.conv.0.0.quantize_weight.running_zero_point torch.Size([960, 1, 1, 1])\n",
            "features.17.conv.0.0.quantize_weight.running_range torch.Size([960, 1, 1, 1])\n",
            "features.17.conv.0.1.weight torch.Size([960])\n",
            "features.17.conv.0.1.bias torch.Size([960])\n",
            "features.17.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
            "features.17.conv.1.0.equ_scale torch.Size([960, 1, 1, 1])\n",
            "features.17.conv.1.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.17.conv.1.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.17.conv.1.0.quantize_weight.running_zero_point torch.Size([960, 1, 1, 1])\n",
            "features.17.conv.1.0.quantize_weight.running_range torch.Size([960, 1, 1, 1])\n",
            "features.17.conv.1.1.weight torch.Size([960])\n",
            "features.17.conv.1.1.bias torch.Size([960])\n",
            "features.17.conv.2.weight torch.Size([320, 960, 1, 1])\n",
            "features.17.conv.2.equ_scale torch.Size([320, 1, 1, 1])\n",
            "features.17.conv.2.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.17.conv.2.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.17.conv.2.quantize_weight.running_zero_point torch.Size([320, 1, 1, 1])\n",
            "features.17.conv.2.quantize_weight.running_range torch.Size([320, 1, 1, 1])\n",
            "features.17.conv.3.weight torch.Size([320])\n",
            "features.17.conv.3.bias torch.Size([320])\n",
            "features.18.0.weight torch.Size([1280, 320, 1, 1])\n",
            "features.18.0.equ_scale torch.Size([1280, 1, 1, 1])\n",
            "features.18.0.quantize_input.running_zero_point torch.Size([1, 1, 1, 1])\n",
            "features.18.0.quantize_input.running_range torch.Size([1, 1, 1, 1])\n",
            "features.18.0.quantize_weight.running_zero_point torch.Size([1280, 1, 1, 1])\n",
            "features.18.0.quantize_weight.running_range torch.Size([1280, 1, 1, 1])\n",
            "features.18.1.weight torch.Size([1280])\n",
            "features.18.1.bias torch.Size([1280])\n",
            "classifier.1.weight torch.Size([10, 1280])\n",
            "classifier.1.bias torch.Size([10])\n",
            "classifier.1.equ_scale torch.Size([10, 1])\n",
            "classifier.1.quantize_input.running_zero_point torch.Size([1])\n",
            "classifier.1.quantize_input.running_range torch.Size([1])\n",
            "classifier.1.quantize_weight.running_zero_point torch.Size([10, 1])\n",
            "classifier.1.quantize_weight.running_range torch.Size([10, 1])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model_xd.named_parameters():\n",
        "    print(name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQceb6IlLcur"
      },
      "outputs": [],
      "source": [
        "model.conv1.name = \"QConv2d\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvFnIQ0PLcus",
        "outputId": "44ce1ac1-b350-4865-9952-fac3afb71875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'QConv2d'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.conv1.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iFWMsyGLcus",
        "outputId": "8f39b313-c90e-44c2-bc13-010a15212f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConv2d(\n",
            "  3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for k, v in cached_input_output.items():\n",
        "    print(k)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDyY_7zZLcut"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
