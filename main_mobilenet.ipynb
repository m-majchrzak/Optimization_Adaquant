{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-majchrzak/Optimization_Adaquant/blob/main/main_mobilenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyu_LNHkLcuQ"
      },
      "source": [
        "## Libraries & Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -r ./Optimization_Adaquant/\n",
        "!git clone https://github.com/m-majchrzak/Optimization_Adaquant.git\n",
        "%cd Optimization_Adaquant/\n",
        "!pip install pyunpack\n",
        "from pyunpack import Archive\n",
        "Archive('calibration_datasets.zip').extractall(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gsj6-ZzeN5_",
        "outputId": "1c92350a-64f8-4dab-8750-420d2777da39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "rm: cannot remove './Optimization_Adaquant/': No such file or directory\n",
            "Cloning into 'Optimization_Adaquant'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 128 (delta 62), reused 70 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 6.31 MiB | 20.72 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "/content/Optimization_Adaquant\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyunpack\n",
            "  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n",
            "Collecting easyprocess (from pyunpack)\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting entrypoint2 (from pyunpack)\n",
            "  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: entrypoint2, easyprocess, pyunpack\n",
            "Successfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LWJOoH8xLcuU"
      },
      "outputs": [],
      "source": [
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "from utils.trainer import Trainer\n",
        "from utils.adaquant import optimize_layer_adaquant\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "import random\n",
        "\n",
        "from utils.quantize import QConv2d, QLinear\n",
        "from utils.load_dataset import load_dataset\n",
        "from utils.resnet import ResNet_imagenet\n",
        "from utils.mobilenet_v2 import mobilenet_v2\n",
        "from utils.misc import set_global_seeds\n",
        "\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nsPl6gZlLcuW"
      },
      "outputs": [],
      "source": [
        "acc = -1\n",
        "loss = -1\n",
        "best_prec1 = 0\n",
        "dtype = torch.float32\n",
        "\n",
        "### SET SEED\n",
        "seed = 123\n",
        "set_global_seeds(seed)\n",
        "\n",
        "device_ids = list(range(torch.cuda.device_count()))\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(device_ids[0])\n",
        "    cudnn.benchmark = True\n",
        "else:\n",
        "    device_ids = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjIYB-L8LcuX"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZtCWrcg2LcuY"
      },
      "outputs": [],
      "source": [
        "# Calib data loading code\n",
        "train_directory='./calibration_datasets/tiny_imagenet/train'\n",
        "train_batch_size = 400\n",
        "train_data = load_dataset(train_directory, train_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyZ0-rUnLcua"
      },
      "source": [
        "## Model, Optimizer, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_VyDDpZPLcua"
      },
      "outputs": [],
      "source": [
        "### CREATE MODEL\n",
        "#model = ResNet_imagenet()\n",
        "model = mobilenet_v2()\n",
        "\n",
        "# define loss function (criterion)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# optimizer configuration\n",
        "optimizer = SGD(model.parameters(),lr=1e-2, momentum=0.5, weight_decay=0)\n",
        "\n",
        "# TRAINER\n",
        "#trainer = Trainer(model, criterion, optimizer, device=torch.device('cuda'))\n",
        "trainer = Trainer(model, criterion, optimizer, device=torch.device('cpu'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1QO1EtnLcuc"
      },
      "source": [
        "## Cache, Hook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "llDYo3MNLcuc"
      },
      "outputs": [],
      "source": [
        "cached_qinput = {}\n",
        "def Qhook(name, module, input, output):\n",
        "    if module not in cached_qinput:\n",
        "        cached_qinput[module] = []\n",
        "        # Meanwhile store data in the RAM.\n",
        "        cached_qinput[module].append(input[0].detach().cpu())\n",
        "        # print(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_y_PDu3wLcud"
      },
      "outputs": [],
      "source": [
        "cached_input_output = {}\n",
        "def hook(name,module, input, output):\n",
        "    if module not in cached_input_output:\n",
        "        cached_input_output[module] = []\n",
        "    # Meanwhile store data in the RAM.\n",
        "    cached_input_output[module].append((input[0].detach().cpu(), output.detach().cpu()))\n",
        "    # print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kl8zpIswLcud",
        "outputId": "c7025283-f196-4233-ac6a-f7c338988bb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features.0.0\n",
            "features.1.conv.0.0\n",
            "features.1.conv.1\n",
            "features.2.conv.0.0\n",
            "features.2.conv.1.0\n",
            "features.2.conv.2\n",
            "features.3.conv.0.0\n",
            "features.3.conv.1.0\n",
            "features.3.conv.2\n",
            "features.4.conv.0.0\n",
            "features.4.conv.1.0\n",
            "features.4.conv.2\n",
            "features.5.conv.0.0\n",
            "features.5.conv.1.0\n",
            "features.5.conv.2\n",
            "features.6.conv.0.0\n",
            "features.6.conv.1.0\n",
            "features.6.conv.2\n",
            "features.7.conv.0.0\n",
            "features.7.conv.1.0\n",
            "features.7.conv.2\n",
            "features.8.conv.0.0\n",
            "features.8.conv.1.0\n",
            "features.8.conv.2\n",
            "features.9.conv.0.0\n",
            "features.9.conv.1.0\n",
            "features.9.conv.2\n",
            "features.10.conv.0.0\n",
            "features.10.conv.1.0\n",
            "features.10.conv.2\n",
            "features.11.conv.0.0\n",
            "features.11.conv.1.0\n",
            "features.11.conv.2\n",
            "features.12.conv.0.0\n",
            "features.12.conv.1.0\n",
            "features.12.conv.2\n",
            "features.13.conv.0.0\n",
            "features.13.conv.1.0\n",
            "features.13.conv.2\n",
            "features.14.conv.0.0\n",
            "features.14.conv.1.0\n",
            "features.14.conv.2\n",
            "features.15.conv.0.0\n",
            "features.15.conv.1.0\n",
            "features.15.conv.2\n",
            "features.16.conv.0.0\n",
            "features.16.conv.1.0\n",
            "features.16.conv.2\n",
            "features.17.conv.0.0\n",
            "features.17.conv.1.0\n",
            "features.17.conv.2\n",
            "features.18.0\n",
            "classifier.1\n"
          ]
        }
      ],
      "source": [
        "for name, m in model.named_modules():\n",
        "    # print(name)\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        print(name)\n",
        "        m.name = name\n",
        "        # print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jQIYG_5dLcue"
      },
      "outputs": [],
      "source": [
        "\n",
        "handlers = []\n",
        "count = 0\n",
        "for name, m in model.named_modules():\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "    #if isinstance(m, Conv2d) or isinstance(m, Linear):\n",
        "    # if isinstance(m, QConv2d):\n",
        "        m.quantize = False\n",
        "        #if count < 10:\n",
        "        # if (isinstance(m, QConv2d) and m.groups == 1) or isinstance(m, QLinear):\n",
        "        handlers.append(m.register_forward_hook(partial(hook,name)))\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7GrF88RKLcuf",
        "outputId": "9c35c806-25f9-44c2-c0a0-8ea580087aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input/outputs cached\n"
          ]
        }
      ],
      "source": [
        "# Store input/output for all quantizable layers\n",
        "trainer.validate(train_data)\n",
        "print(\"Input/outputs cached\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UUS3WM1TLcuf"
      },
      "outputs": [],
      "source": [
        "for handler in handlers:\n",
        "    handler.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7FUABnD0Lcug"
      },
      "outputs": [],
      "source": [
        "for m in model.modules():\n",
        "    if isinstance(m, QConv2d) or isinstance(m, QLinear):\n",
        "        m.quantize = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cbfo-MoXLcug",
        "outputId": "540dcbf2-fdd8-4236-d8b7-92f2d2a837cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([QConv2d(\n",
            "  3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QConv2d(\n",
            "  320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            "), QLinear(\n",
            "  in_features=1280, out_features=1000, bias=True\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")])\n"
          ]
        }
      ],
      "source": [
        "print(cached_input_output.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FiBDSMdLcuh"
      },
      "source": [
        "## Loop Through Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AjR0_09hLcuh"
      },
      "outputs": [],
      "source": [
        "mse_df = pd.DataFrame(index=np.arange(len(cached_input_output)), columns=['name', 'bit', 'shape', 'mse_before', 'mse_after'])\n",
        "print_freq = 100\n",
        "evaluate = \"evaluate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAz_Jm_aLcuh",
        "outputId": "cf98c0e9-31bc-4e01-bf1c-91c2b446cb63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimize 0:features.0.0 for 8 bit of shape torch.Size([32, 3, 3, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:39<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MSE before optimization: 0.36453860998153687\n",
            "MSE after optimization:  0.2530604898929596\n"
          ]
        }
      ],
      "source": [
        "for i, layer in enumerate(cached_input_output):\n",
        "    if i>0: # and seq_adaquant = True\n",
        "        count = 0\n",
        "        cached_qinput = {}\n",
        "        for name, m in model.named_modules():\n",
        "            if layer.name==name:\n",
        "                if count < 1000:\n",
        "                    handler= m.register_forward_hook(partial(Qhook,name))\n",
        "                    count += 1\n",
        "        # Store input/output for all quantizable layers\n",
        "        trainer.validate(train_data)\n",
        "        print(\"cashed quant Input%s\"%layer.name)\n",
        "        cached_input_output[layer][0] = (cached_qinput[layer][0],cached_input_output[layer][0][1])\n",
        "        handler.remove()\n",
        "    print(\"\\nOptimize {}:{} for {} bit of shape {}\".format(i, layer.name, layer.num_bits, layer.weight.shape))\n",
        "\n",
        "    mse_before, mse_after = optimize_layer_adaquant(layer, cached_input_output[layer])\n",
        "\n",
        "    print(\"\\nMSE before optimization: {}\".format(mse_before))\n",
        "    print(\"MSE after optimization:  {}\".format(mse_after))\n",
        "    mse_df.loc[i, 'name'] = layer.name\n",
        "    mse_df.loc[i, 'bit'] = layer.num_bits\n",
        "    mse_df.loc[i, 'shape'] = str(layer.weight.shape)\n",
        "    mse_df.loc[i, 'mse_before'] = mse_before\n",
        "    mse_df.loc[i, 'mse_after'] = mse_after\n",
        "\n",
        "\n",
        "mse_csv = evaluate + '.mse.csv'\n",
        "mse_df.to_csv(mse_csv)\n",
        "\n",
        "filename = evaluate + '.adaquant'\n",
        "torch.save(model.state_dict(), filename)\n",
        "\n",
        "train_data = None\n",
        "cached_input_output = None\n",
        "val_results = trainer.validate(val_data.get_loader())\n",
        "#logging.info(val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQceb6IlLcur"
      },
      "outputs": [],
      "source": [
        "model.conv1.name = \"QConv2d\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvFnIQ0PLcus",
        "outputId": "44ce1ac1-b350-4865-9952-fac3afb71875"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'QConv2d'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.conv1.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iFWMsyGLcus",
        "outputId": "8f39b313-c90e-44c2-bc13-010a15212f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConv2d(\n",
            "  3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n",
            "QConv2d(\n",
            "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "  (quantize_input): QuantThUpdate()\n",
            "  (quantize_weight): QuantThUpdate()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "for k, v in cached_input_output.items():\n",
        "    print(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDyY_7zZLcut"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}